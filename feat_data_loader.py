import tensorflow as tf
import cv2
from argparse import ArgumentParser
from multiprocessing import Process, Queue
import misc

import pickle
import numpy as np

from head_pose.mark_detector import MarkDetector
#from head_pose.os_detector import detect_os
#from head_pose.pose_estimator import PoseEstimator
#from head_pose.stabilizer import Stabilizer

MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)
age_list = ['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']
gender_list = ['Male', 'Female']

def calculate_facial_features(frame, facebox, CNN_INPUT_SIZE, mark_detector, age_net, gender_net, draw_data=False):
    # Detect landmarks from image of 128x128.
    face_img_og = frame[facebox[1]: facebox[3],
                     facebox[0]: facebox[2]]
    face_img = cv2.resize(face_img_og, (CNN_INPUT_SIZE, CNN_INPUT_SIZE))
    face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)

    marks = mark_detector.detect_marks([face_img])

    age_gender_face = cv2.dnn.blobFromImage(face_img_og, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=True)
    gender_net.setInput(age_gender_face)
    gender_preds = gender_net.forward()
    gender = gender_preds[0].argmax()
    #print("Gender : " + gender_list[gender])
    #Predict Age
    age_net.setInput(age_gender_face)
    age_preds = age_net.forward()
    age = age_preds[0].argmax()
    #print("Age Range: " + age_list[age])

    # Convert the marks locations from local CNN to global image.
    marks *= (facebox[2] - facebox[0])
    marks[:, 0] += facebox[0]
    marks[:, 1] += facebox[1]
    
    image_colors = np.zeros((marks.shape[0],3))
    for i in range(len(image_colors)):
        pixcols = frame[int(marks[i, 0]), int(marks[i, 1]), :]
        image_colors[i, :] = pixcols
    
    if draw_data:
        font = cv2.FONT_HERSHEY_SIMPLEX
        fontScale = 1
        fontColor = (0, 255, 0)
        lineType = 2
    
        display_vars = {'age': age_list[age], 'gender': gender_list[gender], 'face_pos_0': marks[0, 0]}
        yloc = 50
        for var_name, value in display_vars.items():
            cv2.putText(frame,'${} = {}'.format(var_name, value),
                (10, yloc),
                font,
                fontScale,
                fontColor,
                lineType)
        yloc += 50
    
        # Uncomment following line to show raw marks.
        mark_detector.draw_marks(
             frame, marks, color=(0, 255, 0))
    
        # Uncomment following line to show facebox.
        mark_detector.draw_box(frame, [facebox])

    return np.concatenate([marks.flatten(), np.array([age, gender]), image_colors.flatten()])
    # Try pose estimation with 68 points.
    #return pose_estimator.solve_pose_by_68_points(marks)

def f_z_generator(CNN_INPUT_SIZE):
    """
    Generates an infinite stream of (z, G(z), F(G(z))) pairs, meaning latent points in the
    GAN's generator's latent space, and the features of the image generated by the GAN.
    """
    # Import official CelebA-HQ networks.
    with open('models/pg_gan/karras2018iclr-celebahq-1024x1024.pkl', 'rb') as file:
        G, D, Gs = pickle.load(file)
    mark_detector = MarkDetector()

    age_net = cv2.dnn.readNetFromCaffe('models/race_age/deploy_age.prototxt', 'models/race_age/age_net.caffemodel')
    gender_net = cv2.dnn.readNetFromCaffe('models/race_age/deploy_gender.prototxt', 'models/race_age/gender_net.caffemodel')
    #pose_estimator = PoseEstimator(img_size=(height, width))
    i = 0
    while True:
        i += 1
        z = misc.random_latents(1, G)
        labels = np.zeros([z.shape[0], 0], np.float32)
        x = G.run(z, labels, out_mul=127.5, out_add=127.5, out_dtype=np.uint8)
        #f = [calculate_facial_features(single_img, CNN_INPUT_SIZE, mark_detector) for single_img in xs]
        #print(x.shape)
        x = np.squeeze(x)
        x = np.transpose(x, (1, 2, 0))
        x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)
        #print("Trying face...")
        facebox = mark_detector.extract_cnn_facebox(x)
        if facebox is None:
            continue
        f = calculate_facial_features(x, facebox, CNN_INPUT_SIZE, mark_detector, age_net, gender_net)
        #print(x.shape)
        #cv2.imwrite("inverter_images/output_{}.png".format(i), x)
        #print("Found face...")
        #f = tf.map_fn(lambda single_img: , x)
        yield (z, x, f.reshape(1, -1))
